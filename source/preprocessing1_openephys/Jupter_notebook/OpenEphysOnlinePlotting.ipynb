{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d64426",
   "metadata": {},
   "source": [
    "### Define & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be828a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "import pandas as pd\n",
    "sys.path.append(r'D:\\Neural-Pipeline\\source')\n",
    "from process_openephys.MemSaccPlot import NeuralDataLoader\n",
    "########### Zarya Trial Settings #############\n",
    "\n",
    "angles = np.zeros(20)\n",
    "eccs = np.zeros(20)\n",
    "\n",
    "eccs[[0, 2, 4, 6]] = 5\n",
    "eccs[[1, 3, 5, 7, 10, 13, 16, 19]] = 10\n",
    "eccs[[8, 11, 14, 17]] = 3.5\n",
    "eccs[[9, 12]] = 6.72\n",
    "eccs[[15, 18]] = 5.83\n",
    "\n",
    "angles[[0, 1]] = 0\n",
    "angles[[2, 3]] = 90\n",
    "angles[[4, 5]] = 180\n",
    "angles[[6, 7]] = 270\n",
    "angles[[8, 9, 10]] = 42\n",
    "angles[[11, 12, 13]] = 138\n",
    "angles[[14, 15, 16]] = 211\n",
    "angles[[17, 18, 19]] = 329   \n",
    "\n",
    "########### Plot Settings #############\n",
    "\n",
    "align_info_list = [{ 'event': 'SACC', 'pre': -0.8, 'post': 0}]\n",
    "align_info = { 'event': 'SACC', 'pre': -0.8, 'post': 0}\n",
    "electrode_id = None\n",
    "kilosort = False\n",
    "\n",
    "###### variable changed by blocks #############\n",
    "\n",
    "\n",
    "data_struct = \"kilosort\"  # Options: \"continuous\", \"spike_time\", \"kilosort\"\n",
    "\n",
    "match data_struct:\n",
    "    case \"continuous\":\n",
    "        base_dir = r\"D:\\20250724\\2025-07-24_13-46-22\\Record Node 110\\experiment3\\recording1\"\n",
    "\n",
    "    case \"spike_time\":\n",
    "        electrode_id = 3\n",
    "        base_dir = r\"D:\\20250724\\2025-07-24_13-46-22\\Record Node 111\\experiment3\\recording1\"\n",
    "    case \"kilosort\":\n",
    "        kilosort = True\n",
    "        datasets = [\n",
    "            # {\n",
    "            #     'base_dir': r\"D:\\20250718\",\n",
    "            #     'kilo_dir': r\"D:\\20250718\\kilosort4\",\n",
    "            #     'sacc_file': 1410\n",
    "            # },\n",
    "            # {\n",
    "            #     'base_dir': r\"D:\\Neural-Pipeline\\data\\20250724_6836\",\n",
    "            #     'kilo_dir': r\"D:\\Neural-Pipeline\\data\\20250724_6836\\kilosort4\",  # Adjust path as needed [1346. 1410. 1412. 1546. 1548. 1550. 1626.]\n",
    "            #     'sacc_file': 1346\n",
    "            # },\n",
    "            {\n",
    "                'base_dir': r\"D:\\Neural-Pipeline\\data\\20250724\",\n",
    "                'kilo_dir': r\"D:\\Neural-Pipeline\\data\\20250724\\kilosort4\",  # Adjust path as needed\n",
    "                'sacc_file': 1702\n",
    "            }\n",
    "            # {\n",
    "            #     'base_dir': r\"D:\\Neural-Pipeline\\data\\20250801\",\n",
    "            #     'kilo_dir': r\"D:\\Neural-Pipeline\\data\\20250801\\kilosort4\",  # Adjust path as needed\n",
    "            #     'sacc_file': 1350\n",
    "            # }\n",
    "        ]\n",
    "\n",
    "    case _:\n",
    "        raise ValueError(f\"Invalid data_struct: {data_struct}. Must be 'continuous', 'spike_time', or 'kilosort'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff9965",
   "metadata": {},
   "source": [
    "### Plot TDI / Tuning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_kilosort_data(base_dir, align_info, compare_position, pdf, save_to_pdf, kilo_dir, sacc_file):\n",
    "    data = NeuralDataLoader(base_dir, kilo_dir = kilo_dir, sacc_file = sacc_file)\n",
    "    data.parse_ttl_events()\n",
    "    \n",
    "    cluster_info = pd.read_csv(data.cluster_info_file, sep='\\t')\n",
    "    all_clusters = cluster_info['cluster_id'].values\n",
    "    cluster_labels = dict(zip(cluster_info['cluster_id'], cluster_info['KSLabel']))\n",
    "    \n",
    "    tdi_all = np.full(len(all_clusters), np.nan)\n",
    "    \n",
    "    for idx, cluster_id in enumerate(all_clusters):\n",
    "        cluster_label = cluster_labels[cluster_id]\n",
    "        print(f\"Processing cluster {cluster_id} ({cluster_label}) ({idx+1}/{len(all_clusters)})\")\n",
    "        \n",
    "        try:\n",
    "            cluster_spikes = data.spike_times[data.cluster_id == cluster_id]\n",
    "            data.current_cluster_spikes = cluster_spikes\n",
    "            \n",
    "            avg_firing_rates, _, tdi = data.avg_firing_rate(align_info)\n",
    "            mean_fr = np.nanmean(list(avg_firing_rates.values()))\n",
    "\n",
    "            print(f\"  Cluster {cluster_id}: TDI={tdi:.3f}, Mean FR={mean_fr:.3f}\")\n",
    "            \n",
    "            if not np.isnan(tdi) and mean_fr > 2 and tdi > 0.45:\n",
    "                tdi_all[idx] = tdi\n",
    "                \n",
    "                depth_str = \"\"\n",
    "                if data.cluster_depth is not None and cluster_id in data.cluster_depth:\n",
    "                    depth = data.cluster_depth[cluster_id]\n",
    "                    depth_str = f\", Depth: {depth:.0f}\"\n",
    "                \n",
    "                # Add cluster type to title\n",
    "                cluster_type = \"Good Unit\" if cluster_label == 'good' else f\"MUA ({cluster_label})\"\n",
    "                title = f\"Cluster {cluster_id} ({cluster_type}) - TDI: {tdi:.2f}{depth_str}, Firing Rate: {mean_fr:.2f}\"\n",
    "                plot_results(data, align_info, avg_firing_rates, compare_position, title, pdf, save_to_pdf)\n",
    "            else:\n",
    "                print(f\"  âœ— Cluster {cluster_id} failed criteria (TDI={tdi:.3f}, FR={mean_fr:.3f})\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing cluster {cluster_id}: {e}\")\n",
    "            \n",
    "    \n",
    "    return tdi_all, data\n",
    "\n",
    "def process_continuous_data(base_dir, align_info, compare_position, pdf, save_to_pdf, electrode_id):\n",
    "    data = NeuralDataLoader(base_dir)\n",
    "    data.load_continuous_data(electrode_id)\n",
    "    data.parse_ttl_events()\n",
    "    \n",
    "    channels_to_process = range(380, 384)  # or [channel] if specific\n",
    "    tdi_all = np.full(len(channels_to_process), np.nan)\n",
    "    \n",
    "    for idx, channel in enumerate(channels_to_process):\n",
    "        print(f\"Processing channel {channel} ({idx+1}/{len(channels_to_process)})\")\n",
    "        \n",
    "        try:\n",
    "            data.get_spike_times(channel, threshold=-65)\n",
    "            total_firing = len(data.spike_times) / (data.ttl_timestamps[-1] - data.ttl_timestamps[0])\n",
    "            \n",
    "            if total_firing >= 1:\n",
    "                avg_firing_rates, _, tdi = data.avg_firing_rate(align_info)\n",
    "                mean_fr = np.nanmean(list(avg_firing_rates.values()))\n",
    "                \n",
    "                if not np.isnan(tdi) and mean_fr > 2 and tdi > 0.45:\n",
    "                    tdi_all[idx] = tdi\n",
    "                    title = f\"Channel {channel} - TDI: {tdi:.2f}, Firing Rate: {mean_fr:.2f}\"\n",
    "                    plot_results(data, align_info, avg_firing_rates, compare_position, title, pdf, save_to_pdf)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing channel {channel}: {e}\")\n",
    "    \n",
    "    return tdi_all\n",
    "\n",
    "def process_electrode_data(base_dir, align_info, compare_position, pdf, save_to_pdf):\n",
    "    tdi_all = np.full(192, np.nan)\n",
    "    \n",
    "    for i in range(96):\n",
    "        electrode_id = i + 1\n",
    "        data = NeuralDataLoader(base_dir, electrode_id)\n",
    "        data.parse_ttl_events()\n",
    "\n",
    "        try:\n",
    "            avg_firing_rates, _, tdi = data.avg_firing_rate(align_info)\n",
    "            mean_fr = np.nanmean(list(avg_firing_rates.values()))\n",
    "            \n",
    "            if not np.isnan(tdi) and mean_fr > 2 and tdi > 0.45:\n",
    "                tdi_all[i] = tdi\n",
    "                title = f\"TDI: {tdi:.2f}, Depth: {10722 - electrode_id * 40}, Firing Rate: {mean_fr:.2f}\"\n",
    "                plot_results(data, align_info, avg_firing_rates, compare_position, title, pdf, save_to_pdf)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing electrode {electrode_id}: {e}\")\n",
    "    \n",
    "    return tdi_all\n",
    "\n",
    "def plot_results(data, align_info, avg_firing_rates, compare_position, title, pdf, save_to_pdf):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    plt.sca(axes[0])\n",
    "    data.plot_avg_firing_rate_heatmap(align_info, avg_firing_rates, eccs, angles)\n",
    "    axes[0].set_title(\"Heatmap\")\n",
    "    \n",
    "    plt.sca(axes[1])\n",
    "    data.plot_psth(avg_firing_rates, compare_position)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    \n",
    "    if save_to_pdf:\n",
    "        pdf.savefig()  \n",
    "        plt.close() \n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c49ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    base_dir = dataset['base_dir']\n",
    "    kilo_dir = dataset['kilo_dir']\n",
    "    sacc_file = dataset['sacc_file']\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing dataset: {base_dir}\")\n",
    "    print(f\"Sacc file: {sacc_file}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    save_to_pdf = True\n",
    "    pdf_path = os.path.join(base_dir, f\"memory_saccade_heatmaps_sacc{sacc_file}.pdf\")\n",
    "    compare_position = 3\n",
    "\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        if kilosort:\n",
    "            tdi_all, data = process_kilosort_data(base_dir, align_info, compare_position, pdf, save_to_pdf, kilo_dir, sacc_file)\n",
    "        elif electrode_id is None:\n",
    "            tdi_all = process_continuous_data(base_dir, align_info, compare_position, pdf, save_to_pdf, electrode_id)\n",
    "        else:\n",
    "            tdi_all = process_electrode_data(base_dir, align_info, compare_position, pdf, save_to_pdf, electrode_id)\n",
    "    \n",
    "    print(f\"Completed processing {base_dir}\")\n",
    "    print(f\"Results saved to: {pdf_path}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e3946",
   "metadata": {},
   "source": [
    "### Plot firing rate by depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a865637",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    base_dir = dataset['base_dir']\n",
    "    kilo_dir = dataset['kilo_dir']\n",
    "    sacc_file = dataset['sacc_file']\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing dataset: {base_dir}\")\n",
    "    print(f\"Sacc file: {sacc_file}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load data and process everything in one call\n",
    "    data = NeuralDataLoader(base_dir, kilo_dir=kilo_dir, sacc_file=sacc_file,  tdi_threshold = 0.45)\n",
    "    \n",
    "    # Process all units and create comprehensive heatmap\n",
    "    results = data.process_all_units_comprehensive(align_info)\n",
    "    tdi_all, fr_all, depth_all, cluster_data_all, top_matrix, bottom_matrix, unit_info, time_bins = results\n",
    "    \n",
    "    print(f\"Completed processing {base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot of TDI over depth\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(tdi_all, depth_all, alpha=0.6, s=50, c=tdi_all, cmap='viridis')\n",
    "\n",
    "# Add threshold line\n",
    "plt.axvline(x=0.55, color='red', linestyle='--', linewidth=2, alpha=0.8, label='TDI threshold = 0.55')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(label='TDI Value')\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xlabel('TDI (Tuning Depth Index)', fontsize=12)\n",
    "plt.ylabel('Depth (Î¼m)', fontsize=12)\n",
    "plt.title(f'TDI vs Depth Distribution\\n{os.path.basename(base_dir)} - {len(tdi_all)} units', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Invert y-axis so depth=0 is at bottom, max depth at top\n",
    "# plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "tdi_plot_path = os.path.join(base_dir, f\"tdi_vs_depth_sacc{sacc_file}.png\")\n",
    "plt.savefig(tdi_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"TDI vs Depth plot saved to: {tdi_plot_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65178cd6",
   "metadata": {},
   "source": [
    "### Plot TDI by depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "319d16cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing dataset: D:\\Neural-Pipeline\\data\\20250724\n",
      "Sacc file: 1702\n",
      "==================================================\n",
      "Selected base directory: D:\\Neural-Pipeline\\data\\20250724\n",
      "Loading kilosort data...\n",
      "Filtering TTL data for block 1702\n",
      "Parsed 176 trials, 84 good trials.\n",
      "Processing TDI of cluster: 0\n",
      "Processing TDI of cluster: 1\n",
      "Processing TDI of cluster: 2\n",
      "Processing TDI of cluster: 3\n",
      "Processing TDI of cluster: 4\n",
      "Processing TDI of cluster: 5\n",
      "Processing TDI of cluster: 6\n",
      "Processing TDI of cluster: 7\n",
      "Processing TDI of cluster: 8\n",
      "Processing TDI of cluster: 9\n",
      "Processing TDI of cluster: 10\n",
      "Processing TDI of cluster: 11\n",
      "Processing TDI of cluster: 14\n",
      "Processing TDI of cluster: 12\n",
      "Processing TDI of cluster: 13\n",
      "Processing TDI of cluster: 15\n",
      "Processing TDI of cluster: 16\n",
      "Processing TDI of cluster: 17\n",
      "Processing TDI of cluster: 18\n",
      "Processing TDI of cluster: 20\n",
      "Processing TDI of cluster: 19\n",
      "Processing TDI of cluster: 21\n",
      "Processing TDI of cluster: 22\n",
      "Processing TDI of cluster: 23\n",
      "Processing TDI of cluster: 25\n",
      "Processing TDI of cluster: 28\n",
      "Processing TDI of cluster: 27\n",
      "Processing TDI of cluster: 36\n",
      "Processing TDI of cluster: 26\n",
      "Processing TDI of cluster: 35\n",
      "Processing TDI of cluster: 34\n",
      "Processing TDI of cluster: 37\n",
      "Processing TDI of cluster: 38\n",
      "Processing TDI of cluster: 24\n",
      "Processing TDI of cluster: 30\n",
      "Processing TDI of cluster: 32\n",
      "Processing TDI of cluster: 39\n",
      "Processing TDI of cluster: 29\n",
      "Processing TDI of cluster: 31\n",
      "Processing TDI of cluster: 33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m NeuralDataLoader(base_dir, kilo_dir\u001b[38;5;241m=\u001b[39mkilo_dir, sacc_file\u001b[38;5;241m=\u001b[39msacc_file,  tdi_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.45\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Process all units and create comprehensive heatmap\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_comprehensive_tdi_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43malign_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Neural-Pipeline\\source\\process_openephys\\MemSaccPlot.py:869\u001b[0m, in \u001b[0;36mNeuralDataLoader.create_comprehensive_tdi_heatmap\u001b[1;34m(self, event_name)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(condition_rates) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    868\u001b[0m     rates_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(condition_rates\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m--> 869\u001b[0m     r_max, r_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmax(rates_list), \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrates_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;66;03m# Calculate SSE (sum of squared errors)\u001b[39;00m\n\u001b[0;32m    872\u001b[0m     sse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnansum([condition_vars[c] \u001b[38;5;241m*\u001b[39m condition_trial_counts[c] \n\u001b[0;32m    873\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m condition_rates\u001b[38;5;241m.\u001b[39mkeys()])\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnanmin\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\fetschlab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:350\u001b[0m, in \u001b[0;36mnanmin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m    346\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll-NaN slice encountered\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[0;32m    347\u001b[0m                       stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;66;03m# Slow, but safe for subclasses of ndarray\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     a, mask \u001b[38;5;241m=\u001b[39m \u001b[43m_replace_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m     res \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mamin(a, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fetschlab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:107\u001b[0m, in \u001b[0;36m_replace_nan\u001b[1;34m(a, val)\u001b[0m\n\u001b[0;32m    104\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     np\u001b[38;5;241m.\u001b[39mcopyto(a, val, where\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a, mask\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    base_dir = dataset['base_dir']\n",
    "    kilo_dir = dataset['kilo_dir']\n",
    "    sacc_file = dataset['sacc_file']\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing dataset: {base_dir}\")\n",
    "    print(f\"Sacc file: {sacc_file}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load data and process everything in one call\n",
    "    data = NeuralDataLoader(base_dir, kilo_dir=kilo_dir, sacc_file=sacc_file,  tdi_threshold = 0.45)\n",
    "    \n",
    "    # Process all units and create comprehensive heatmap\n",
    "    results = data.create_comprehensive_tdi_heatmap(align_info)\n",
    "    \n",
    "    print(f\"Completed processing {base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fedeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdi_baseline_corrected, valid_units, time_bins = results\n",
    "print(tdi_baseline_corrected[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
